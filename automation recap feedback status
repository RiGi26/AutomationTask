import logging
import os
from io import BytesIO
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

import msoffcrypto
import numpy as np
import pandas as pd


@dataclass
class ProcessingConfig:
    """Configuration class for MOXA data processing"""
    path_folder: Path = Path("D:\\Daily MOXA")
    file_raw: Path = Path(r"D:\Daily MOXA\Leads MOXA\Template Moxa Konven dan Moxa Syariah (30 Mei 2025).xlsx")
    date: str = '30/05/2025'
    file_password: str = "202505"
    file_moxa: Path = Path(r"D:\Daily MOXA\Data Leads MOXA.xlsx")
    output_filename: str = 'Data Leads MOXA.xlsx'

    # Column definitions
    required_cols: List[str] = None
    call_result_columns: List[str] = None
    update_columns: List[str] = None
    sheet_names: List[str] = None
    raw_names: List[str] = None

    def __post_init__(self):
        """Initialize column lists if not provided"""
        if self.required_cols is None:
            self.required_cols = [
                'Id User Profile', 'Id Leads Data User', 'hasil call 1', 'hasil call 2',
                'hasil call 3', 'hasil call 1.1', 'hasil call 2.1', 'hasil call 3.1',
                'hasil call 1.2', 'hasil call 2.2', 'hasil call 3.2'
            ]

        if self.call_result_columns is None:
            self.call_result_columns = [
                'hasil call 1', 'hasil call 2', 'hasil call 3', 'hasil call 1.1',
                'hasil call 2.1', 'hasil call 3.1', 'hasil call 1.2', 'hasil call 2.2',
                'hasil call 3.2'
            ]

        if self.update_columns is None:
            self.update_columns = [
                'tanggal call 1', 'hasil call 1', 'tanggal call 2', 'hasil call 1.1',
                'tanggal call 3', 'hasil call 1.2'
            ]

        if self.sheet_names is None:
            self.sheet_names = ['NMC', 'NMC SY']

        if self.raw_names is None:
            self.raw_names = ['Template Konven', 'Template Syariah']


class MOXADataProcessor:
    """Enhanced MOXA Data Processor with improved structure and error handling"""

    def __init__(self, config: Optional[ProcessingConfig] = None):
        self.config = config or ProcessingConfig()
        self.logger = self._setup_logging()
        self._validate_configuration()

    def _setup_logging(self) -> logging.Logger:
        """Configure logging with proper formatting"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)

    def _validate_configuration(self) -> None:
        """Validate configuration and file existence"""
        try:
            if not self.config.file_raw.exists():
                raise FileNotFoundError(f"Raw data file not found: {self.config.file_raw}")

            if not self.config.file_moxa.exists():
                raise FileNotFoundError(f"Master MOXA file not found: {self.config.file_moxa}")

            if not self.config.path_folder.exists():
                self.config.path_folder.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"Created output directory: {self.config.path_folder}")

        except Exception as e:
            self.logger.error(f"Configuration validation failed: {e}")
            raise

    def decrypt_excel_file(self, file_path: Path, password: str) -> BytesIO:
        """Decrypt password-protected Excel file"""
        try:
            with open(file_path, "rb") as f:
                office_file = msoffcrypto.OfficeFile(f)
                office_file.load_key(password=password)
                decrypted = BytesIO()
                office_file.decrypt(decrypted)
                decrypted.seek(0)
                return decrypted
        except Exception as e:
            self.logger.error(f"Failed to decrypt file {file_path}: {e}")
            raise

    def load_sheet_data(self, decrypted_file: BytesIO, sheet_name: str) -> pd.DataFrame:
        """Load data from decrypted Excel file"""
        try:
            df = pd.read_excel(decrypted_file, sheet_name=sheet_name, engine='openpyxl')
            self.logger.info(f"Loaded {len(df)} rows from sheet '{sheet_name}'")
            return df
        except Exception as e:
            self.logger.error(f"Failed to load sheet '{sheet_name}': {e}")
            raise

    def filter_by_date(self, df: pd.DataFrame, target_date: str) -> pd.DataFrame:
        """Filter data by target date"""
        try:
            # Ensure date column is datetime
            if not pd.api.types.is_datetime64_any_dtype(df['tanggal penarikan']):
                df['tanggal penarikan'] = pd.to_datetime(
                    df['tanggal penarikan'],
                    format='%d/%m/%Y',
                    errors='coerce'
                )

            target_dt = pd.to_datetime(target_date, format='%d/%m/%Y')
            filtered_df = df[df['tanggal penarikan'] == target_dt].copy()

            self.logger.info(f"Filtered to {len(filtered_df)} rows for date {target_date}")
            return filtered_df

        except Exception as e:
            self.logger.error(f"Date filtering failed: {e}")
            raise

    def clean_today_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and prepare today's data"""
        try:
            # Rename columns
            df = df.rename(columns={
                'Transaction Date Time': 'Transaction',
                'dihubungi ': 'dihubungi'
            })

            # Drop unnecessary columns
            columns_to_drop = [
                'Is Syariah', 'hasil call 2', 'hasil call 3', 'hasil call 2.1',
                'hasil call 3.1', 'hasil call 2.2', 'hasil call 3.2', 'CALL',
                'Unnamed: 65'
            ]

            df = df.drop(columns=columns_to_drop, errors='ignore')
            self.logger.info(f"Cleaned data: {len(df)} rows, {len(df.columns)} columns")
            return df

        except Exception as e:
            self.logger.error(f"Data cleaning failed: {e}")
            raise

    def process_call_results(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process and cascade call results"""
        try:
            df_processed = df.copy()

            # Fill NaN values and clean data
            df_processed[self.config.call_result_columns] = df_processed[self.config.call_result_columns].fillna(
                'missing')

            for col in self.config.call_result_columns:
                df_processed[col] = (
                    df_processed[col]
                    .astype(str)
                    .str.replace(r'\(DOUBLE\)', '', regex=True)
                    .str.strip()
                )

            # Apply cascading logic
            cascade_rules = [
                ('hasil call 3', 'hasil call 2'),
                ('hasil call 2', 'hasil call 1'),
                ('hasil call 3.1', 'hasil call 2.1'),
                ('hasil call 2.1', 'hasil call 1.1'),
                ('hasil call 3.2', 'hasil call 2.2'),
                ('hasil call 2.2', 'hasil call 1.2')
            ]

            for source, target in cascade_rules:
                mask = df_processed[source] != 'missing'
                df_processed.loc[mask, target] = df_processed.loc[mask, source]

            # Replace 'missing' back to NaN
            df_processed[self.config.call_result_columns] = df_processed[self.config.call_result_columns].replace(
                {'missing': np.nan})

            self.logger.info("Call results processing completed")
            return df_processed

        except Exception as e:
            self.logger.error(f"Call results processing failed: {e}")
            raise

    def handle_duplicates(self, df: pd.DataFrame, id_column: str = 'Id Leads Data User') -> pd.DataFrame:
        """Handle duplicate records in DataFrame"""
        try:
            duplicate_count = df[id_column].duplicated().sum()

            if duplicate_count > 0:
                self.logger.warning(f"Found {duplicate_count} duplicates in {id_column}, removing...")
                df = df.drop_duplicates(subset=id_column, keep='last')
                self.logger.info(f"After deduplication: {len(df)} rows remaining")

            return df

        except Exception as e:
            self.logger.error(f"Duplicate handling failed: {e}")
            raise

    def merge_and_update_data(self, master_df: pd.DataFrame, update_df: pd.DataFrame) -> pd.DataFrame:
        """Merge master data with updates using robust approach"""
        try:
            # Handle duplicates
            master_df = self.handle_duplicates(master_df)
            update_df = self.handle_duplicates(update_df)

            # Create update mapping
            update_data = update_df[['Id Leads Data User'] + self.config.update_columns].set_index('Id Leads Data User')

            # Merge data
            merged_df = master_df.merge(
                update_data,
                left_on='Id Leads Data User',
                right_index=True,
                how='left',
                suffixes=('', '_update')
            )

            # Update columns with new values where they exist
            for col in self.config.update_columns:
                update_col = f"{col}_update"
                if update_col in merged_df.columns:
                    merged_df[col] = merged_df[update_col].fillna(merged_df[col])
                    merged_df.drop(columns=[update_col], inplace=True)

            self.logger.info(f"Data merge completed: {len(merged_df)} rows")
            return merged_df

        except Exception as e:
            self.logger.error(f"Data merge failed: {e}")
            raise

    def standardize_phone_numbers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Standardize phone number format"""
        try:
            if 'Phone' in df.columns:
                df['Phone'] = (
                    df['Phone']
                    .astype(str)
                    .str.replace(r'^(?!0)', '0', regex=True)
                )
                self.logger.info("Phone numbers standardized")

            if 'Nomor KTP' in df.columns:
                df['Nomor KTP'] = df['Nomor KTP'].astype(str).fillna("")

            return df

        except Exception as e:
            self.logger.error(f"Phone number standardization failed: {e}")
            raise

    def process_single_sheet(self, sheet_name: str, raw_name: str) -> pd.DataFrame:
        """Process a single sheet of data"""
        try:
            self.logger.info(f"Processing sheet: {sheet_name} with raw template: {raw_name}")

            # Decrypt and load raw data
            decrypted_file = self.decrypt_excel_file(self.config.file_raw, self.config.file_password)
            raw_data = self.load_sheet_data(decrypted_file, raw_name)

            # Load master data
            master_data = pd.read_excel(self.config.file_moxa, sheet_name=sheet_name)

            # Filter and clean today's data
            today_data = self.filter_by_date(raw_data, self.config.date)
            today_data = self.clean_today_data(today_data)

            # Concatenate master and today's data
            combined_data = pd.concat([master_data, today_data], ignore_index=True)
            combined_data['tanggal penarikan'] = pd.to_datetime(
                combined_data['tanggal penarikan'],
                format='%d/%m/%Y',
                errors='coerce'
            )

            # Process call results
            processed_raw = self.process_call_results(raw_data)

            # Extract result status for updates
            result_status = processed_raw[
                ['Id User Profile', 'Id Leads Data User', 'tanggal call 1', 'hasil call 1',
                 'tanggal call 2', 'hasil call 1.1', 'tanggal call 3', 'hasil call 1.2']
            ]

            # Merge and update data
            final_data = self.merge_and_update_data(combined_data, result_status)

            # Standardize data
            final_data = self.standardize_phone_numbers(final_data)

            self.logger.info(f"Sheet {sheet_name} processing completed: {len(final_data)} rows")
            return final_data

        except Exception as e:
            self.logger.error(f"Sheet processing failed for {sheet_name}: {e}")
            raise

    def autofit_columns(self, writer, sheet_name: str, df: pd.DataFrame) -> None:
        """Auto-fit column widths in Excel"""
        try:
            worksheet = writer.sheets[sheet_name]

            for i, col in enumerate(df.columns):
                max_length = max(
                    df[col].astype(str).str.len().max(),
                    len(str(col))
                )
                adjusted_width = min(max_length + 2, 50)
                worksheet.set_column(i, i, adjusted_width)

        except Exception as e:
            self.logger.error(f"Column autofit failed: {e}")

    def export_to_excel(self, all_data: Dict[str, pd.DataFrame]) -> None:
        """Export processed data to Excel with formatting"""
        try:
            output_path = self.config.path_folder / self.config.output_filename

            with pd.ExcelWriter(output_path, engine='xlsxwriter',
                                datetime_format='dd/mm/yyyy') as writer:

                for sheet_name, df in all_data.items():
                    df.to_excel(writer, sheet_name=sheet_name, index=False)

                    # Apply formatting
                    self.autofit_columns(writer, sheet_name, df)

                    # Add header formatting
                    workbook = writer.book
                    worksheet = writer.sheets[sheet_name]

                    header_format = workbook.add_format({
                        'bold': True,
                        'text_wrap': True,
                        'valign': 'top',
                        'fg_color': '#D7E4BC',
                        'border': 1
                    })

                    for col_num, value in enumerate(df.columns.values):
                        worksheet.write(0, col_num, value, header_format)

            self.logger.info(f"Data exported successfully to {output_path}")

        except Exception as e:
            self.logger.error(f"Export failed: {e}")
            raise

    def validate_processed_data(self, data: Dict[str, pd.DataFrame]) -> None:
        """Validate processed data quality"""
        try:
            for sheet_name, df in data.items():
                if df.empty:
                    raise ValueError(f"Sheet {sheet_name} is empty")

                required_columns = ['Id User Profile', 'Id Leads Data User']
                missing_cols = [col for col in required_columns if col not in df.columns]
                if missing_cols:
                    raise ValueError(f"Missing required columns in {sheet_name}: {missing_cols}")

                # Check for high null percentages
                null_percentages = df.isnull().sum() / len(df) * 100
                high_null_cols = null_percentages[null_percentages > 80]

                if not high_null_cols.empty:
                    self.logger.warning(f"High null percentages in {sheet_name}: {high_null_cols.to_dict()}")

            self.logger.info("Data validation completed successfully")

        except Exception as e:
            self.logger.error(f"Data validation failed: {e}")
            raise

    def process_all_data(self) -> Dict[str, pd.DataFrame]:
        """Main processing method - orchestrates the entire workflow"""
        try:
            self.logger.info("Starting MOXA data processing...")

            # Set pandas display options
            pd.set_option('display.max_columns', None)
            pd.set_option('display.width', 2000)

            all_data = {}

            # Process each sheet
            for sheet_name, raw_name in zip(self.config.sheet_names, self.config.raw_names):
                processed_data = self.process_single_sheet(sheet_name, raw_name)
                all_data[sheet_name] = processed_data

            # Validate processed data
            self.validate_processed_data(all_data)

            # Export results
            self.export_to_excel(all_data)

            self.logger.info("MOXA data processing completed successfully!")
            return all_data

        except Exception as e:
            self.logger.error(f"Processing failed: {e}")
            raise


def main():
    """Main execution function"""
    try:
        # Initialize processor with default configuration
        processor = MOXADataProcessor()

        # Process all data
        results = processor.process_all_data()

        print("Processing completed successfully!")
        print(f"Processed {len(results)} sheets:")
        for sheet_name, df in results.items():
            print(f"  - {sheet_name}: {len(df)} rows")

    except Exception as e:
        print(f"Processing failed: {e}")
        raise


if __name__ == "__main__":
    main()
